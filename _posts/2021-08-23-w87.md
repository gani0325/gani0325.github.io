---
title: 2_DeepNoid CNN 기초
categories: [writing] 
comments: true
---
<p><span style="border-bottom: 12px solid #dcf1fb; padding: 0 0 0 0.2em;">DeepNoid 수업을 참고하여 작성하였습니다</span></p>
<p><span style="border-bottom: 12px solid #dcf1fb; padding: 0 0 0 0.2em;">CNN 기초</span></p>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>정의</title>
</head>
<body>

<pre>
</pre>

<p><span style="background: linear-gradient(to right, #ffa7a3, #5673bd); padding: 0.43em 1em; font-size: 19px; border-radius: 3px; color: #ffffff;">CNN</span></p>

<pre>
1. CNN 이란 (Convolutional Neural Network)
   : 합성공 신경망이란? 
   - 이미지를 날것 그대로 받아, convolutional layer을 통해 이미지에서
    공간적, 지역적 특징 추출(extraction)을 진행
   - task 에 따라 classification, segmentation, detection 으로 나뉜다
   - task는 정답(lavel)을 필요로 하는 지도학습이기에 그에 맞는 데이터 필요
  
   : 처음에는 작게 -> 다 본후 더 넓게 
   -> 좁은 영역부터 넓은 영역으로 시야 넓히며 관찰

   : 주요 용어 정리
   - 필터 (커널)  
   - 필터의 크기 (kernel size)
   - 필터가 한번에 보이는 영역 (Receptive field)
   - 필터가 움직인 거리 (stride)

   : 필터 크기를 줄이는 대신 이미지 크기를 줄이기도 한다
   - 이미지 크기 줄이는 방법 (pooling)
   - pooling 을 통해 각 결과 값 (feature map)의 dimensionality를 축소하는
    과정을 거친다
    
2. CNN 핵심 개녕
   : feature extraction 과정
     1) 훑어보기 Conv2D (fliter =, kernel_size =, strides =, padding =)
     2) 정렬하기 BatchNormalization
      - 유의미한 신호는 남기고, 불필요 신호는 버려 원활한 신호 전달 도움
     3) 신호 변환 ACT(RELU)
      - Sigmoide function의 output은 0, 1 극과 극으로 신경망이 깊어지게 되면
        gradient vanishing 현상이 발생하기에
      - ReLU 와 같은 non-linear 한 Activation function 사용
     4) 이미지 줄이기 POOR
      - filter 개수를 가진 합성곱 레이어를 거치면 tensor 사이즈가
        computing source 에 부담이 되고 합성곱 레이어의 핵심은
        이미지 특징을 추출하는 것이기에, feature map 사이즈 줄여준다
     5) 끝
      - 이후 모든 노드를 일렬로 편 후 최종 판단 flatten

3. 신경망의 학습 과정
  1) 데이터 (X)를 입력한다
  2) 초기값으로 연산하며, Convolutional layer 를 지나간다 (순전파)
  3) Dense layer 까지 지나가 예측 값 (y)이 나온다
  4) 지정한 손실 함수 (loss function)을 통해서 예측 값과 실제 값(label)
    간의 loss(손실, 차이) 계산한다
  5) 지정한 optimizer로 예측 값과 실제 값 차이를 줄여나간다 (역전파)
    - 이 과정에서 가중치 (weight)가 업데이트 된다
  6) 설정한 epoch 혹은 step 수 만큼 학습 진행한다

4. Hyper Parameter
   : 신경망모델을 구성할 때, 모델의 결과에 영향을 끼칠 수 있는 변수
   : 모델 내부의 구성을 바꿀 수 있는 변수를 parameter
   : 모델 외부에서 바꿀 수 있는 변수를 hyper-parameter
      - epoch : 모든 데이터를 순전파, 역전파 하여 가중치, 편항 업데이트 하는 횟수 (=학습횟수)
      - batch size : 데이터를 몇개 단위로 묶는지
      - learning rate : 학습을 진행하는 rate, 일반적으로 0.01 진행
      - loss function : 실제값(label), 예측 값(y)의 오차 계산하는 함수
      - optimizer : loss function을 최적하는 방법

   1) loss function (손실 함수)
    - 실제 값(label)과 모델을 통해 나온 예측 값(y)간의 차이를 줄여주는 함수
    - 오차(loss)가 크면 좋지 않은 모델,
    - loss 값 최소화 하는 것이 신경망의 목표
    - loss 값을 줄일 수 있는 가중치(weight), 편향(bias) 줄이는 것이 목표
    - 학습하고자 하는 task에 따라 다르며,
    - 회귀를 위한 문제에는 MSE(평균 제곱 오차),
    - 분류를 위한 문제에는 Cross-entropy

   2) optimizer (최적화 기법)
    - 손실함수를 줄여갈 수 있는 방법
    - 경사 하강법, 배치 경사 하강법, 확률적 경사 하강법, 미니 배치 경사 하강법 ...
    - 어떤 optimizer 를 사용할 지 모를 땐, adam!

5. 데이터 형태
  - classification 
   1) classification 모델의 input, label 데이터 형태
      - input (image)
      - label (정답)
   2) binary classification 모델의 예측 방식
      - 0, 1 중 정답은 1(Abdomen)
   3) Multionmial classification 모델의 예측 방식
      - f1-score, sensivity, precision, accuracy

  - segmentation
   1) segmentation 모델의 input, label 데이터 형태
      - input (image)
      - label (정답)

  - Detection
   1) detection 모델의 input, label 데이터 형태
      - input (image)
      - label (정답)

</pre>
</body>
</html>
