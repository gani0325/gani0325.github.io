---
title: 4_DeepNoid 머신러닝
categories: [writing] 
comments: true
---
<p><span style="border-bottom: 12px solid #dcf1fb; padding: 0 0 0 0.2em;">DeepNoid 수업을 참고하여 작성하였습니다</span></p>
<p><span style="border-bottom: 12px solid #dcf1fb; padding: 0 0 0 0.2em;">머신러닝</span></p>

<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>정의</title>
</head>
<body>

<pre>
</pre>

<p><span style="background: linear-gradient(to right, #ffa7a3, #5673bd); padding: 0.43em 1em; font-size: 19px; border-radius: 3px; color: #ffffff;">AI</span></p>

<pre>
1. AI
    - 인공지능 : 기계가 사람의 행동을 모방하게 하는 기술
    - 기계학습 : 기계가 일일이 코드로 명시하지 않은 동작을
    데이터로부터 학습
    - 빅데이터 : 기존의 데이터베이스로는 수집, 저장, 분석 등을
    수행하기 어려울 만큼의 방대한 데이터

2. 머신러닝
    - 대량의 데이터, 훈련
    - 대량의 복잡한 데이터셋 다루기 용이
    - 컴퓨터가 알고리즘 안에서 데이터와 결과 값을 보고
      스스로 훈련하여 규칙을 찾아냄 
    - 유형
      1) 지도학습 : 정답을 알려주며 모델을 학습
       ex) Classification, Regression
      2) 비지도학습 : 정답이 없는 데이터를 통해 직접 규칙을 발견하도록 하는 방식 
       ex) Clustering, Assocication Rules

1) 지도학습
   - k-최근접 이웃법
     : 새로운 데이터를 입력받았을 때, 가장 가까이 있는 클래스의 개수로
      분류하는 직관적인 알고리즘
      -> 데이터 간의 거리는 주로 유클리디안 거리 활용

   - Decision Tree
     : 한번에 하나의 설명변수를 통해 예측 가능한 규칙들의 집합을
      생성하는 알고리즘
      -> 질문을 계속 던지면서 대상을 좁혀나가는 스무고개 놀이 개념   
     : 장점
       - 시각화하기 쉽다
       - 분류 오류율이 가장 낮은 지저므올 0이 될 떄까지 계속해서 분류
       - 분류 오류 측정 지표 : 지니계쑤, 교차 엔트로피
     : 단점
        - 다른 머신러닝 기법에 비해 낮은 정확도
        - 적절한 prunning  지점을 선택못할 시에 overfitting 위험
        -> 여러 의사 결정 나무 모델을 만들어서 결과 집계하자! (예측을 좀더 일반화!)


    - Ensemble Methold
      1) 배깅 (Boostrap Agrregating, Bagging)
       : 의사결정나무를 개선시키기 위해 Boostrap을 활용하여 다수의 의사결정나무 생성 (overfitting 개선)
       -> input에서 여러번 랜덤샘플링하여 만든 모델 집계
       : 부스트래핑 - 복원 추출을 활용한 random sampling의 한 기법

       2) 랜덤포레스트 (Random Forest)
       : 설명 변수 또한 랜덤 샘플링하여 각 의사결정 나무들 간의 상관성을 제거하는 방법
       : 각 모델에서는 전체 설명변수 중 루트(전체설명변수개수)만큼의
       변수만 랜덤으로 사용 

       3) 부스팅 (Boosting)
       : 데이터에 하나의 트리 모델을 학습 시킨 결과를 통해
       다음 트리 모델을 학습시키는 방식
         -> 다수의 트리 모델이 순차적으로 학습되는 형태
       : 기존 배깅 모델과의 차이
          - 배깅, 랜덤포레스트 : 각각의 트리가 독립적
          - Boosting : 각각의 트리가 시퀀스를 가짐

    - Neural Network
        1) 인공신경망
        : 신경망 - 여러 자극들이 신경세포로 들어오고 어느정도
         이상의 자극이 들어오면 이를 축상을 통해 전달
        : 요소
            - 입력층 중간층 출력층
            - 입력 -> 입력가중합 ->   

        2) 활성화함수
         -> 입력신호의 총합을 출력신호로 변환하는 함수 

        3) 서포트 벡터머신
        : 결정 경계를 통해 분류를 위한 기준 선을 정의하는 모델
        : 분류되지 않은새로운 점이 나타나면 경계의 어느 쪽에 속하는지 확인
         -> 최적의 분리 초평면을 선택하는 방법 

    - Supplort Vector Machine
      1) 최대 마진 분류기
        : 훈련 관측치로부터 가장 멀리 떨어진 분리 초평면을 선택
        -> 분리 초평면으롭퉈 마진이 가장 큰 위치 찾기
        : 초평면 - p차원 공간에서 p-1 인 평평한 아핀 부분공간
        : 마진 - 계싼되는 관측치들에서 초평면까지의 가장 짧은 거리
      
      2) 서포트 벡터 분류기
        : 대부분의 경우 분리 초평면이 존재하기 어려움 (정확히 분리 어려움)
        : 최대 마진 분류기 역시 존재 어려움
         -> 소프트 마진을 사용하여 클래스를 거의 분류하는 초평면 찾자
        : 소프트 마진 - 마진 평면을 넘어가는 인스턴스를 일정 허용하는 분류기
         -> Slack Variable의 허용 개수인 조율 파라미터가 존재 
      
      3) 서포트 벡터 머신
        : 비선형 결정 경계를 그려 분류하는 방법
        : Kernel Trick을 통해 더 높은 차원으로의 mapping function
         -> 구불구불한 dicision boundary로 분류한다고 생각하기

1) 비지도학습 
    - Clustering
      1) 계층 분석
        : 비슷한 군집끼리 묶어 가면서 최종적으로 하나의 케이스가 될 때까지 군집을 묶는 알고르지므
        : 반복적으로 가장 가까운 거리에 있는 데이터를 서로 묶어줌
        : 장점 - 군집의 개수를 미리 정해주지 않아도 됨 

       2) k-means clustering
        : 측정값들이 가능한 한 동질적이도록 미리 정해 놓은 k개의
       군집이 중첩되지 않도록 표본을 나누는 군집법
        : k개 초기 클러스터 시작 -> 각 레코드는 가장 가까운 중심이
       속한 클러스터에 할당 -> 한 관측치가 빠지거나 추가되면 군집들의
       중심을 다시 계산하고, 2번 반복 -> 각 관측치에 대한 클러스터의
       변화가 없으면 중지
        : 최적의 클러스터 개수 (k) 구하기
          - 팔꿈치 방법 : 평균 거리가 갑자기 떨어지는 k 값 선택
          - 실루엣 방법 : 실루엣 평균 값을 보고 k 값 선택

    - Association Rules
        1) 연관규칙
         : A가 발생하면 B도 발생하더라 라는 형태의 연관성 찾기
         : 지지도 - 전체 거래 건수 중 X와 Y 모두 포함하는 거래건수
         :  신뢰도 - X를 포함하는 거래 중에 Y도 포함하는 거래비율
         : 항상도 - X가 주어지지 않을 떄의 Y확률 대비 X가 주어졌을 때 Y의 확률 증가 비율


</pre>
</body>
</html>
